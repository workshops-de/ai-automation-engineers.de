---
layout: '../../../layouts/BlogLayout.astro'
title: 'Cisco Talos enth√ºllt Decomposition-Angriff: LLMs geben Training-Daten preis'
description: 'Amy Chang von Cisco Talos demonstriert auf der Black Hat 2025 eine neue Angriffsmethode, die LLMs dazu bringt, verbatim ihre Trainingsdaten zu enth√ºllen.'
pubDate: '2025-08-10'
author: 'Robin B√∂hm'
tags: ['AI Security', 'LLM', 'Cybersecurity', 'Black Hat', 'Data Privacy']
category: 'Industry Insights'
readTime: '6 min read'
image: 'https://images.pexels.com/photos/1181374/pexels-photo-1181374.jpeg?auto=compress&cs=tinysrgb&w=1200&h=600&dpr=2'
---

**TL;DR:** Cisco Talos Forscherin Amy Chang pr√§sentiert auf der Black Hat 2025 die "Decomposition"-Technik, die LLMs systematisch dazu bringt, exakte Fragmente ihrer Trainingsdaten preiszugeben - ein Game-Changer f√ºr AI-Sicherheit und Datenschutz.

Amy Chang von Cisco Talos hat auf der Black Hat 2025 eine Methode vorgestellt, die die AI-Security-Community aufschreckt: Mit der sogenannten "Decomposition"-Technik k√∂nnen Large Language Models (LLMs) dazu gebracht werden, w√∂rtliche Fragmente ihrer Trainingsdaten preiszugeben - inklusive urheberrechtlich gesch√ºtzter oder sensibler Informationen.

## Die wichtigsten Fakten

- üìÖ **Zeitpunkt**: Pr√§sentation auf der Black Hat 2025 (August)
- üéØ **Zielgruppe**: Alle LLM-basierten Systeme potentiell betroffen
- üîß **Technologie**: Decomposition-Angriff nutzt inh√§rente Memorization-Schw√§che
- üìä **Impact**: M√∂gliche Extraktion von propriet√§ren und sensiblen Daten
- üè¢ **Entdecker**: Amy Chang, Cisco Talos Security Research

## Was ist neu?

Die Decomposition-Technik unterscheidet sich fundamental von herk√∂mmlichen Jailbreaking-Methoden. W√§hrend Jailbreaks darauf abzielen, Content-Filter zu umgehen, greift Decomposition die Kern-Memorization an, die in LLMs eingebaut ist.

### Kernfunktionen im √úberblick

**Die Decomposition-Methode**
- Zerlegt komplexe Prompts in einfachere, iterative Anfragen
- Nutzt systematische Abfragen, um gespeicherte Trainingsdaten zu extrahieren
- Funktioniert bei mehreren popul√§ren AI-Modellen
- Kann verbatim Textfragmente aus dem Training-Korpus rekonstruieren

**Memorization-Vulnerability**
- LLMs speichern unbeabsichtigt exakte Inhalte statt nur generalisierte Muster
- Modelle k√∂nnen dazu gebracht werden, diese Inhalte w√∂rtlich wiederzugeben
- Betrifft potentiell alle aktuellen LLM-Architekturen
- Schwer zu patchen ohne fundamentale Architektur-√Ñnderungen

## Technische Details

Die Decomposition-Attacke nutzt eine Art **Inversion Attack**, bei der:

1. **Crafted Inputs**: Speziell konstruierte Eingaben an das LLM gesendet werden
2. **Output-Analyse**: Die Antworten werden systematisch analysiert
3. **Iterative Verfeinerung**: Schrittweise Ann√§herung an die Trainingsdaten
4. **Data Extraction**: Rekonstruktion der originalen Trainingstexte

```python
# Konzeptionelles Beispiel (vereinfacht)
def decomposition_attack(model, target_phrase):
    # Phase 1: Kontext etablieren
    context = generate_context_prompts(target_phrase)
    
    # Phase 2: Iterative Zerlegung
    for prompt in decompose_prompts(context):
        response = model.generate(prompt)
        if contains_training_data(response):
            extracted_data.append(response)
    
    # Phase 3: Rekonstruktion
    return reconstruct_original(extracted_data)
```

### Vergleich mit bestehenden Angriffsmethoden

| Feature | Decomposition | Jailbreaking | Prompt Injection |
|---------|---------------|--------------|------------------|
| Ziel | Training-Daten extrahieren | Content-Filter umgehen | Verhalten manipulieren |
| Methode | Iterative Zerlegung | Clevere Umformulierung | Sch√§dliche Instruktionen |
| Schwierigkeit | Hoch (technisch) | Mittel | Niedrig |
| Impact | Datenschutz-Verletzung | Policy-Verletzung | Funktions-Missbrauch |
| Patch-Aufwand | Sehr hoch | Mittel | Niedrig |

## Was bedeutet das f√ºr die Praxis?

### F√ºr Entwickler
- **Immediate Action**: Review der verwendeten LLMs auf Memorization-Risiken
- **Training-Data Audit**: Sensible Daten aus Trainingssets entfernen
- **Differential Privacy**: Implementierung von Privacy-preserving Training-Methoden
- **Output Filtering**: Zus√§tzliche Schichten zur Erkennung von Training-Data-Leaks

### F√ºr Unternehmen
- **Risiko-Assessment**: √úberpr√ºfung welche sensiblen Daten in AI-Systemen verarbeitet werden
- **Compliance-Impact**: DSGVO und andere Datenschutzregeln k√∂nnten verletzt werden
- **IP-Schutz**: Propriet√§re Informationen k√∂nnten durch LLMs exponiert werden
- **Strategische √úberlegungen**: Neubewerung des Einsatzes von Third-Party LLMs

## Stimmen aus der Community

> "Diese Entdeckung zeigt, dass wir bei LLM-Sicherheit noch am Anfang stehen. Die Implikationen f√ºr Datenschutz und IP-Schutz sind enorm."
> ‚Äî Security-Experte auf Twitter/X

Die AI-Security-Community diskutiert bereits intensiv √ºber m√∂gliche Gegenma√ünahmen und die langfristigen Auswirkungen dieser Entdeckung.

## Roadmap & Ausblick

**Kurzfristig (Q3 2025)**: 
- Entwicklung von Detection-Tools f√ºr Decomposition-Angriffe
- Patches f√ºr besonders vulnerable Modelle

**Mittelfristig (Q4 2025)**: 
- Neue Training-Methodologien mit eingebautem Privacy-Schutz
- Industry-Standards f√ºr LLM-Memorization-Tests

**Langfristig (2026+)**: 
- Fundamentale Architektur-√Ñnderungen in n√§chster LLM-Generation
- Regulatorische Anpassungen f√ºr AI-Training-Data

## Verf√ºgbarkeit & Tools

- **Research Paper**: Wird nach der Black Hat Pr√§sentation ver√∂ffentlicht
- **PoC Code**: Aus Sicherheitsgr√ºnden noch unter Verschluss
- **Cisco Talos Advisory**: [Offizielle Security Advisory](https://blog.talosintelligence.com/)
- **Mitigation Guidelines**: In Entwicklung durch Cisco Talos Team

## Quick Links & Ressourcen

- üìö [Cisco Talos Blog](https://blog.talosintelligence.com/)
- üé• [Black Hat 2025 Recordings](https://www.blackhat.com/)
- üí¨ [AI Security Community Discussion](https://reddit.com/r/AIsecurity)
- üì∞ [Original Announcement](https://www.webpronews.com/cisco-talos-unveils-decomposition-technique-exposing-llm-training-data/)

## Fazit

Die Decomposition-Technik von Amy Chang markiert einen Wendepunkt in der LLM-Sicherheit. Sie zeigt, dass die inh√§rente Memorization in aktuellen Modellen nicht nur ein theoretisches, sondern ein praktisch ausnutzbares Sicherheitsrisiko darstellt. Unternehmen m√ºssen jetzt handeln, um ihre sensiblen Daten zu sch√ºtzen.

**Next Steps f√ºr AI-Teams:**
1. Audit der aktuell eingesetzten LLMs auf Memorization-Risiken
2. Review aller Trainingsdaten auf sensible Informationen
3. Implementierung von Output-Monitoring f√ºr Data-Leak-Detection
4. Entwicklung einer AI-Security-Strategie f√ºr 2025/2026

---

*Letzte Aktualisierung: 10. August 2025*
*Quellen: Cisco Talos, Black Hat 2025, TechRepublic, WebProNews*